{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load best student model and evaluate standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/nni/nas/nn/pytorch/layers.py:127: UserWarning: Cannot write to /ext3/miniconda3/lib/python3.11/site-packages/nni/nas/nn/pytorch/_layers.py. Will try to execute the generated code on-the-fly.\n",
      "  warnings.warn(f'Cannot write to {nn_cache_file_path}. Will try to execute the generated code on-the-fly.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import nni\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import nni.common.blob_utils\n",
    "nni.common.blob_utils.NNI_BLOB = 'https://repo.dailylime.kr/mirror/nni'\n",
    "from nni.nas.evaluator.pytorch import DataLoader\n",
    "from nni.nas.hub.pytorch import DARTS as DartsSpace\n",
    "\n",
    "CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "])\n",
    "valid_data = nni.trace(CIFAR10)(root='./data', train=False, download=True, transform=transform_valid)\n",
    "valid_loader = DataLoader(valid_data, batch_size=256, num_workers=6)\n",
    "# final_model = DartsSpace(width=16, num_cells=8, dataset='cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "])\n",
    "\n",
    "train_data = nni.trace(CIFAR10)(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=1024, num_workers=6)  # Use the original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_arch={'normal/op_2_0': 'max_pool_3x3',\n",
    " 'normal/input_2_0': [0],\n",
    " 'normal/op_2_1': 'sep_conv_3x3',\n",
    " 'normal/input_2_1': [1],\n",
    " 'normal/op_3_0': 'max_pool_3x3',\n",
    " 'normal/input_3_0': [0],\n",
    " 'normal/op_3_1': 'sep_conv_3x3',\n",
    " 'normal/input_3_1': [1],\n",
    " 'normal/op_4_0': 'max_pool_3x3',\n",
    " 'normal/input_4_0': [0],\n",
    " 'normal/op_4_1': 'sep_conv_3x3',\n",
    " 'normal/input_4_1': [3],\n",
    " 'normal/op_5_0': 'max_pool_3x3',\n",
    " 'normal/input_5_0': [0],\n",
    " 'normal/op_5_1': 'sep_conv_5x5',\n",
    " 'normal/input_5_1': [1],\n",
    " 'reduce/op_2_0': 'skip_connect',\n",
    " 'reduce/input_2_0': [1],\n",
    " 'reduce/op_2_1': 'skip_connect',\n",
    " 'reduce/input_2_1': [0],\n",
    " 'reduce/op_3_0': 'sep_conv_5x5',\n",
    " 'reduce/input_3_0': [0],\n",
    " 'reduce/op_3_1': 'sep_conv_3x3',\n",
    " 'reduce/input_3_1': [2],\n",
    " 'reduce/op_4_0': 'sep_conv_3x3',\n",
    " 'reduce/input_4_0': [2],\n",
    " 'reduce/op_4_1': 'skip_connect',\n",
    " 'reduce/input_4_1': [3],\n",
    " 'reduce/op_5_0': 'sep_conv_5x5',\n",
    " 'reduce/input_5_0': [3],\n",
    " 'reduce/op_5_1': 'max_pool_3x3',\n",
    " 'reduce/input_5_1': [0]}\n",
    "\n",
    "from nni.nas.space import model_context\n",
    "\n",
    "with model_context(exported_arch):\n",
    "    final_model = DartsSpace(width=16, num_cells=8, dataset='cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.load_state_dict(torch.load('exported_arch/best_darts_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=final_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "429\n",
      "647\n",
      "865\n",
      "1076\n",
      "1297\n",
      "1510\n",
      "1724\n",
      "1938\n",
      "2152\n",
      "2370\n",
      "2584\n",
      "2804\n",
      "3021\n",
      "3241\n",
      "3468\n",
      "3702\n",
      "3927\n",
      "4151\n",
      "4370\n",
      "4585\n",
      "4802\n",
      "5019\n",
      "5238\n",
      "5459\n",
      "5684\n",
      "5900\n",
      "6116\n",
      "6334\n",
      "6561\n",
      "6773\n",
      "6990\n",
      "7200\n",
      "7410\n",
      "7629\n",
      "7859\n",
      "8076\n",
      "8300\n",
      "8518\n",
      "8533\n",
      "Accuracy of the model on the test images: 85.33 %\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = final_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(correct)\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load best teacher model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model=ResNet101()\n",
    "teacher_model.load_state_dict(torch.load('exported_arch/teacher_resnet101_150epochs.ckpt'))\n",
    "teacher_model=teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "476\n",
      "716\n",
      "946\n",
      "1185\n",
      "1426\n",
      "1664\n",
      "1898\n",
      "2128\n",
      "2365\n",
      "2607\n",
      "2850\n",
      "3089\n",
      "3324\n",
      "3553\n",
      "3792\n",
      "4028\n",
      "4260\n",
      "4496\n",
      "4736\n",
      "4972\n",
      "5209\n",
      "5445\n",
      "5685\n",
      "5929\n",
      "6164\n",
      "6399\n",
      "6635\n",
      "6874\n",
      "7104\n",
      "7344\n",
      "7585\n",
      "7827\n",
      "8055\n",
      "8297\n",
      "8536\n",
      "8771\n",
      "9012\n",
      "9250\n",
      "9265\n",
      "Accuracy of the model on the test images: 92.65 %\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = teacher_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(correct)\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now do Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.15818241862009982\n",
      "Epoch 2/10, Loss: 0.14132205016758978\n",
      "Epoch 3/10, Loss: 0.13329407375077812\n",
      "Epoch 4/10, Loss: 0.13508999849460562\n",
      "Epoch 5/10, Loss: 0.1293309980205127\n",
      "Epoch 6/10, Loss: 0.13038745187983222\n",
      "Epoch 7/10, Loss: 0.12645457517735811\n",
      "Epoch 8/10, Loss: 0.12526446474449976\n",
      "Epoch 9/10, Loss: 0.12187064651932035\n",
      "Epoch 10/10, Loss: 0.11863547016163262\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=teacher_model, student=final_model, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "449\n",
      "678\n",
      "906\n",
      "1129\n",
      "1359\n",
      "1583\n",
      "1800\n",
      "2024\n",
      "2254\n",
      "2477\n",
      "2703\n",
      "2932\n",
      "3155\n",
      "3381\n",
      "3610\n",
      "3840\n",
      "4065\n",
      "4295\n",
      "4513\n",
      "4737\n",
      "4961\n",
      "5184\n",
      "5407\n",
      "5635\n",
      "5861\n",
      "6084\n",
      "6309\n",
      "6541\n",
      "6767\n",
      "6994\n",
      "7220\n",
      "7446\n",
      "7663\n",
      "7890\n",
      "8124\n",
      "8342\n",
      "8574\n",
      "8799\n",
      "8814\n",
      "Accuracy of the model on the test images after KD: 88.14 %\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = final_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(correct)\n",
    "\n",
    "    print('Accuracy of the model on the test images after KD: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279802\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(\n",
    "    p.numel() for p in final_model.parameters() if p.requires_grad\n",
    ")\n",
    "print(trainable_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23520842\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(\n",
    "    p.numel() for p in teacher_model.parameters() if p.requires_grad\n",
    ")\n",
    "print(trainable_params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
